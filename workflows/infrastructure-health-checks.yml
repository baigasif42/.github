name: Infrastructure Health Checks

on:
  schedule:
    # Infrastructure checks every 4 hours
    - cron: '0 */4 * * *'
    # Daily comprehensive check at 2 AM UTC  
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      check_type:
        description: 'Type of infrastructure check'
        required: true
        default: 'standard'
        type: choice
        options:
        - standard
        - comprehensive
        - azure-only
        - kubernetes-only
        - debug

env:
  DISCORD_WEBHOOK: https://discord.com/api/webhooks/1391803646500671588/7_zWESR3N1FtkNIu-vsAKdicx8JTjdlBYW0WPLvkr7VK3IaRRefqy2zW7eqEwoDqF5Cx

jobs:
  azure-infrastructure-health:
    runs-on: ubuntu-latest
    if: github.event.inputs.check_type != 'kubernetes-only'
    outputs:
      status: ${{ steps.azure_check.outcome }}
      health_score: ${{ steps.azure_check.outputs.health_score }}
      summary: ${{ steps.azure_check.outputs.summary }}
      critical_issues: ${{ steps.azure_check.outputs.critical_issues }}
      debug_info: ${{ steps.azure_check.outputs.debug_info }}
    steps:
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: '{"clientId":"${{ secrets.AZURE_CLIENT_ID }}","clientSecret":"${{ secrets.AZURE_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZURE_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZURE_TENANT_ID }}"}'
          
      - name: Check Azure Infrastructure
        id: azure_check
        run: |
          # Continue on errors to collect comprehensive health data
          set +e
          
          echo "Starting Azure infrastructure health check..."
          echo "Current subscription: $(az account show --query name -o tsv 2>/dev/null || echo 'Unknown')"
          echo "Subscription ID: $(az account show --query id -o tsv 2>/dev/null || echo 'Unknown')"
          
          # Initialize tracking
          total_checks=0
          failed_checks=0
          critical_issues=()
          debug_info=()
          
          # Function to safely check resource status
          check_component() {
            local component_name="$1"
            local check_command="$2"
            local expected_status="$3"
            
            ((total_checks++))
            echo "Checking $component_name..."
            
            # Execute the check command and capture output and errors
            local actual_status
            local error_output
            actual_status=$(eval "$check_command" 2>/tmp/error_output.txt)
            local exit_code=$?
            
            if [ $exit_code -ne 0 ]; then
              error_output=$(cat /tmp/error_output.txt 2>/dev/null || echo "Unknown error")
              
              # Check if it's a "not found" error (acceptable for optional resources)
              if [[ "$error_output" == *"was not found"* ]] || [[ "$error_output" == *"ResourceNotFound"* ]]; then
                echo "⚠️ $component_name: Resource not found (may be optional)"
                debug_info+=("$component_name: Resource not found - $error_output")
                # Don't count as failure for optional resources
                return 0
              else
                echo "❌ $component_name: Failed to check (Error: $error_output)"
                ((failed_checks++))
                critical_issues+=("$component_name: Check failed")
                debug_info+=("$component_name: Check failed - $error_output")
                return 1
              fi
            fi
            
            # Validate the returned status
            if [ -z "$actual_status" ] || [ "$actual_status" = "null" ]; then
              echo "⚠️ $component_name: No status returned"
              debug_info+=("$component_name: Empty status returned")
              return 0
            fi
            
            # Compare with expected status
            if [ "$actual_status" = "$expected_status" ]; then
              echo "✅ $component_name: $actual_status"
              debug_info+=("$component_name: OK ($actual_status)")
            else
              echo "❌ $component_name: $actual_status (Expected: $expected_status)"
              ((failed_checks++))
              critical_issues+=("$component_name: $actual_status")
              debug_info+=("$component_name: FAILED - got '$actual_status', expected '$expected_status'")
            fi
          }
          
          echo ""
          echo "=== RESOURCE GROUPS VERIFICATION ==="
          echo "Available resource groups:"
          az group list --query "[].{Name:name, Status:properties.provisioningState}" -o table 2>/dev/null || echo "Failed to list resource groups"
          
          echo ""
          echo "=== AKS CLUSTERS ==="
          
          # Check AKS clusters (these are critical resources)
          check_component "Dev AKS Cluster" \
            "az aks show --name rewardsy-dev-aks --resource-group rewardsy-dev-rg --query 'powerState.code' -o tsv" \
            "Running"
          
          check_component "Prod AKS Cluster" \
            "az aks show --name rewardsy-prod-aks --resource-group rewardsy-prod-rg --query 'powerState.code' -o tsv" \
            "Running"
          
          echo ""
          echo "=== CONTAINER REGISTRIES ==="
          
          # Check ACRs (critical for deployments)
          check_component "Dev Container Registry" \
            "az acr show --name rewardsydevacr --query 'provisioningState' -o tsv" \
            "Succeeded"
          
          check_component "Prod Container Registry" \
            "az acr show --name rewardsyprodacr --query 'provisioningState' -o tsv" \
            "Succeeded"
          
          echo ""
          echo "=== SERVICE BUS ==="
          
          # Check Service Bus namespaces (critical for microservices communication)
          check_component "Dev Service Bus" \
            "az servicebus namespace show --name rewardsy-dev-servicebus --resource-group rewardsy-dev-rg --query 'status' -o tsv" \
            "Active"
          
          check_component "Prod Service Bus" \
            "az servicebus namespace show --name rewardsy-prod-servicebus --resource-group rewardsy-prod-rg --query 'status' -o tsv" \
            "Active"
          
          echo ""
          echo "=== STORAGE ACCOUNTS ==="
          
          # Check storage accounts (critical for application data)
          check_component "Dev Storage" \
            "az storage account show --name rewardsydevstg --query 'statusOfPrimary' -o tsv" \
            "available"
          
          check_component "Prod Storage" \
            "az storage account show --name rewardsyprodstg --query 'statusOfPrimary' -o tsv" \
            "available"
          
          check_component "Dev Infra Storage" \
            "az storage account show --name rewardsydevinfrastg --query 'statusOfPrimary' -o tsv" \
            "available"
          
          check_component "Prod Infra Storage" \
            "az storage account show --name rewardsyprodinfrastg --query 'statusOfPrimary' -o tsv" \
            "available"
          
          # Note: MongoDB clusters are not checked as they don't exist in current infrastructure
          # Based on local testing, these resources don't exist:
          # - rewardsy-dev-mongo-cluster
          # - rewardsy-prod-mongo-cluster
          
          echo ""
          echo "=== OPTIONAL RESOURCES ==="
          echo "Note: MongoDB clusters not checked (confirmed non-existent in infrastructure)"
          
          # Calculate health score
          if [ $total_checks -eq 0 ]; then
            health_score=0
          else
            health_score=$(( (total_checks - failed_checks) * 100 / total_checks ))
          fi
          
          # Create summary
          summary="Health Score: ${health_score}%, Checked: $total_checks components, Issues: $failed_checks"
          
          # Format critical issues for output
          critical_list=""
          if [ ${#critical_issues[@]} -gt 0 ]; then
            critical_list=$(printf "%s; " "${critical_issues[@]}")
            critical_list=${critical_list%; }
          fi
          
          # Format debug info for output
          debug_list=""
          if [ ${#debug_info[@]} -gt 0 ]; then
            debug_list=$(printf "%s\n" "${debug_info[@]}")
          fi
          
          # Set outputs using proper multiline format
          {
            echo "health_score=$health_score"
            echo "summary=$summary"
            echo "critical_issues=$critical_list"
            echo "debug_info<<EOF"
            echo "$debug_list"
            echo "EOF"
          } >> $GITHUB_OUTPUT
          
          echo ""
          echo "=== AZURE HEALTH SUMMARY ==="
          echo "Health Score: $health_score%"
          echo "Components Checked: $total_checks"
          echo "Critical Issues Found: $failed_checks"
          
          if [ ${#critical_issues[@]} -gt 0 ]; then
            echo "Critical Issues:"
            printf '  - %s\n' "${critical_issues[@]}"
          fi
          
          # Debug mode output
          if [ "${{ github.event.inputs.check_type }}" = "debug" ]; then
            echo ""
            echo "=== DEBUG INFORMATION ==="
            printf '%s\n' "${debug_info[@]}"
          fi
          
          # Only fail if health score is critically low (less than 80% for production)
          if [ $health_score -lt 80 ]; then
            echo "Health check failed: Health score $health_score% is below acceptable threshold (80%)"
            exit 1
          fi
          
          echo "Azure infrastructure health check completed successfully with score: $health_score%"

  kubernetes-health:
    runs-on: ubuntu-latest
    if: github.event.inputs.check_type != 'azure-only'
    outputs:
      status: ${{ steps.k8s_check.outcome }}
      pod_summary: ${{ steps.k8s_check.outputs.pod_summary }}
      namespace_details: ${{ steps.k8s_check.outputs.namespace_details }}
      debug_info: ${{ steps.k8s_check.outputs.debug_info }}
    steps:
      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'
          
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: '{"clientId":"${{ secrets.AZURE_CLIENT_ID }}","clientSecret":"${{ secrets.AZURE_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZURE_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZURE_TENANT_ID }}"}'
          
      - name: Check Kubernetes Health
        id: k8s_check
        run: |
          # Continue on errors to collect comprehensive data
          set +e
          
          echo "Starting Kubernetes health check..."
          
          # Based on infrastructure docs, these are the expected namespaces
          namespaces=("rewardsy-webapp" "rewardsy-backend" "rewardsy-merchant" "rewardsy-data" "rewardsy-admin")
          
          total_pods=0
          running_pods=0
          failed_pods=0
          namespace_status=()
          debug_info=()
          cluster_checked=""
          
          # Function to safely check cluster connectivity
          check_cluster() {
            local cluster_name="$1"
            local resource_group="$2"
            local environment="$3"
            
            echo "=== $environment CLUSTER ==="
            
            # Try to connect to cluster
            if ! az aks get-credentials --resource-group "$resource_group" --name "$cluster_name" --overwrite-existing 2>/dev/null; then
              echo "❌ Cannot get credentials for $cluster_name"
              debug_info+=("$environment: Failed to get credentials for $cluster_name")
              return 1
            fi
            
            if ! kubectl cluster-info --request-timeout=10s &>/dev/null; then
              echo "❌ Cannot connect to $cluster_name"
              debug_info+=("$environment: Cannot connect to cluster $cluster_name")
              return 1
            fi
            
            echo "✅ Connected to $cluster_name"
            debug_info+=("$environment: Successfully connected to $cluster_name")
            
            # Check for additional namespaces dynamically
            local all_namespaces
            all_namespaces=$(kubectl get namespaces -o json 2>/dev/null | jq -r '.items[].metadata.name' | grep "^rewardsy" 2>/dev/null || echo "")
            if echo "$all_namespaces" | grep -q "rewardsy-data-analytics"; then
              if [[ ! " ${namespaces[@]} " =~ " rewardsy-data-analytics " ]]; then
                namespaces+=("rewardsy-data-analytics")
                echo "✅ Found additional namespace: rewardsy-data-analytics"
                debug_info+=("$environment: Found additional namespace: rewardsy-data-analytics")
              fi
            fi
            
            # Check each namespace
            local env_total=0
            local env_running=0
            local env_failed=0
            
            for ns in "${namespaces[@]}"; do
              echo ""
              echo "Checking namespace: $ns"
              
              if kubectl get namespace "$ns" &>/dev/null; then
                # Safe arithmetic with proper validation
                local ns_total_raw ns_running_raw ns_failed_raw
                
                ns_total_raw=$(kubectl get pods -n "$ns" --no-headers 2>/dev/null | wc -l || echo "0")
                ns_running_raw=$(kubectl get pods -n "$ns" --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l || echo "0")
                ns_failed_raw=$(kubectl get pods -n "$ns" --no-headers 2>/dev/null | grep -c -E "(Failed|Error|CrashLoopBackOff|Pending)" 2>/dev/null || echo "0")
                
                # Ensure we have valid numbers
                local ns_total="${ns_total_raw:-0}"
                local ns_running="${ns_running_raw:-0}"
                local ns_failed="${ns_failed_raw:-0}"
                
                # Validate numbers before arithmetic operations
                if [[ ! "$ns_total" =~ ^[0-9]+$ ]]; then ns_total=0; fi
                if [[ ! "$ns_running" =~ ^[0-9]+$ ]]; then ns_running=0; fi
                if [[ ! "$ns_failed" =~ ^[0-9]+$ ]]; then ns_failed=0; fi
                
                # Safe arithmetic operations
                env_total=$((env_total + ns_total))
                env_running=$((env_running + ns_running))
                env_failed=$((env_failed + ns_failed))
                
                echo "  Pods: $ns_running/$ns_total running, $ns_failed failed"
                namespace_status+=("$environment-$ns: $ns_running/$ns_total running")
                debug_info+=("$environment-$ns: $ns_running/$ns_total running, $ns_failed failed")
                
                # Log problematic pods for debugging
                if [ $ns_failed -gt 0 ]; then
                  local problematic_pods
                  problematic_pods=$(kubectl get pods -n "$ns" --no-headers 2>/dev/null | grep -E "(Failed|Error|CrashLoopBackOff|Pending)" | awk '{print $1}' 2>/dev/null || echo "")
                  if [ -n "$problematic_pods" ]; then
                    debug_info+=("$environment-$ns: Problematic pods: $problematic_pods")
                  fi
                fi
                
              else
                echo "  ⚠️ Namespace not found"
                namespace_status+=("$environment-$ns: not found")
                debug_info+=("$environment-$ns: Namespace not found")
              fi
            done
            
            echo "  $environment summary: $env_running/$env_total running, $env_failed failed"
            
            # Update global counters (prioritize production, fallback to dev)
            if [ "$environment" = "PRODUCTION" ] || [ -z "$cluster_checked" ]; then
              total_pods=$env_total
              running_pods=$env_running
              failed_pods=$env_failed
              cluster_checked="$environment"
            fi
            
            return 0
          }
          
          # Check production cluster first (primary)
          if check_cluster "rewardsy-prod-aks" "rewardsy-prod-rg" "PRODUCTION"; then
            echo ""
            echo "Production cluster accessible, also checking development cluster..."
            check_cluster "rewardsy-dev-aks" "rewardsy-dev-rg" "DEVELOPMENT" || echo "Development cluster check failed (non-critical)"
          else
            echo ""
            echo "Production cluster failed, trying development cluster as fallback..."
            if ! check_cluster "rewardsy-dev-aks" "rewardsy-dev-rg" "DEVELOPMENT"; then
              echo "❌ Both clusters failed to connect"
              debug_info+=("CRITICAL: Both production and development clusters are unreachable")
            fi
          fi
          
          # Create summaries
          if [ $total_pods -eq 0 ]; then
            pod_summary="No pods found in accessible clusters"
          else
            pod_summary="$cluster_checked: $running_pods/$total_pods running"
            if [ $failed_pods -gt 0 ]; then
              pod_summary="$pod_summary (Failed: $failed_pods)"
            fi
          fi
          
          # Format namespace details
          namespace_details=""
          if [ ${#namespace_status[@]} -gt 0 ]; then
            namespace_details=$(printf "%s | " "${namespace_status[@]}")
            namespace_details=${namespace_details% | }
          fi
          
          # Format debug info
          debug_list=""
          if [ ${#debug_info[@]} -gt 0 ]; then
            debug_list=$(printf "%s\n" "${debug_info[@]}")
          fi
          
          # Set outputs using proper multiline format
          {
            echo "pod_summary=$pod_summary"
            echo "namespace_details=$namespace_details"
            echo "debug_info<<EOF"
            echo "$debug_list"
            echo "EOF"
          } >> $GITHUB_OUTPUT
          
          echo ""
          echo "=== KUBERNETES SUMMARY ==="
          echo "Pod Summary: $pod_summary"
          echo "Namespace Details: $namespace_details"
          
          # Debug mode output
          if [ "${{ github.event.inputs.check_type }}" = "debug" ]; then
            echo ""
            echo "=== DEBUG INFORMATION ==="
            printf '%s\n' "${debug_info[@]}"
          fi
          
          # Failure conditions
          if [ -z "$cluster_checked" ]; then
            echo "Kubernetes health check failed: No clusters accessible"
            exit 1
          elif [ $total_pods -gt 0 ]; then
            # Calculate failure percentage
            local failure_percentage=0
            if [ $total_pods -gt 0 ]; then
              failure_percentage=$((failed_pods * 100 / total_pods))
            fi
            
            if [ $failure_percentage -gt 50 ]; then
              echo "Kubernetes health check failed: $failure_percentage% of pods have issues (threshold: 50%)"
              exit 1
            fi
          fi
          
          echo "Kubernetes infrastructure health check completed successfully"

  monitoring-check:
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 2 * * *' || github.event.inputs.check_type == 'comprehensive'
    outputs:
      status: ${{ steps.monitoring_check.outcome }}
      monitoring_summary: ${{ steps.monitoring_check.outputs.monitoring_summary }}
    steps:
      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'
          
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: '{"clientId":"${{ secrets.AZURE_CLIENT_ID }}","clientSecret":"${{ secrets.AZURE_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZURE_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZURE_TENANT_ID }}"}'
          
      - name: Check Monitoring Stack
        id: monitoring_check
        run: |
          echo "Checking monitoring stack..."
          
          monitoring_issues=0
          cluster_connected=false
          
          # Try to connect to any available cluster
          for cluster_info in "rewardsy-prod-aks:rewardsy-prod-rg:PRODUCTION" "rewardsy-dev-aks:rewardsy-dev-rg:DEVELOPMENT"; do
            IFS=':' read -r cluster_name resource_group env_name <<< "$cluster_info"
            
            echo "Trying $env_name cluster: $cluster_name"
            
            if az aks get-credentials --resource-group "$resource_group" --name "$cluster_name" --overwrite-existing &>/dev/null; then
              if kubectl cluster-info --request-timeout=10s &>/dev/null; then
                echo "✅ Connected to $env_name cluster"
                cluster_connected=true
                break
              fi
            fi
          done
          
          if [ "$cluster_connected" = false ]; then
            echo "❌ Cannot connect to any cluster for monitoring check"
            echo "monitoring_summary=Cannot connect to clusters for monitoring check" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          echo ""
          echo "=== ELK STACK ==="
          
          # Check ELK components across all namespaces
          elk_components=("elasticsearch" "kibana" "logstash" "filebeat")
          elk_healthy=0
          elk_total=0
          
          for component in "${elk_components[@]}"; do
            pods_raw=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | grep "$component" | wc -l || echo "0")
            running_raw=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | grep "$component" | grep -c "Running" || echo "0")
            
            # Ensure valid numbers
            pods="${pods_raw:-0}"
            running="${running_raw:-0}"
            
            if [[ ! "$pods" =~ ^[0-9]+$ ]]; then pods=0; fi
            if [[ ! "$running" =~ ^[0-9]+$ ]]; then running=0; fi
            
            elk_total=$((elk_total + pods))
            elk_healthy=$((elk_healthy + running))
            
            echo "$component: $running/$pods running"
            
            if [ $pods -gt 0 ] && [ $running -lt $pods ]; then
              ((monitoring_issues++))
            fi
          done
          
          echo ""
          echo "=== EXTERNAL ENDPOINTS ==="
          
          # Test Kibana accessibility (based on infrastructure docs)
          kibana_status="000"
          for kibana_url in "https://app.rewardsy.one/kibana/" "https://dev.app.rewardsy.one/kibana/"; do
            status=$(curl -s -o /dev/null -w "%{http_code}" --max-time 10 "$kibana_url" 2>/dev/null || echo "000")
            if [ "$status" = "200" ] || [ "$status" = "302" ] || [ "$status" = "401" ]; then
              kibana_status="$status"
              echo "✅ Kibana accessible at $kibana_url (HTTP $status)"
              break
            fi
          done
          
          if [ "$kibana_status" = "000" ]; then
            echo "❌ Kibana not accessible from any URL"
            ((monitoring_issues++))
          fi
          
          # Create monitoring summary
          monitoring_summary="ELK: $elk_healthy/$elk_total healthy, Kibana: HTTP $kibana_status, Issues: $monitoring_issues"
          
          echo "monitoring_summary=$monitoring_summary" >> $GITHUB_OUTPUT
          
          echo ""
          echo "=== MONITORING SUMMARY ==="
          echo "$monitoring_summary"
          
          # Only fail if monitoring is completely broken (more than 3 issues)
          if [ $monitoring_issues -gt 3 ]; then
            echo "Monitoring check failed: $monitoring_issues issues detected (threshold: 3)"
            exit 1
          fi

  send-notifications:
    runs-on: ubuntu-latest
    needs: [azure-infrastructure-health, kubernetes-health, monitoring-check]
    if: always()
    steps:
      - name: Send Azure Infrastructure Notification
        if: needs.azure-infrastructure-health.result != 'skipped'
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ env.DISCORD_WEBHOOK }}
          username: The Sentinel
          status: ${{ needs.azure-infrastructure-health.outputs.status }}
          description: |
            **${{ needs.azure-infrastructure-health.outputs.status == 'success' && '✅ Success' || '❌ Failure' }}: Azure Infrastructure Health Check**
            
            ${{ needs.azure-infrastructure-health.outputs.status == 'failure' && '@everyone **CRITICAL INFRASTRUCTURE ISSUE**' || '' }}
            
            **Health Summary**
            ${{ needs.azure-infrastructure-health.outputs.summary || 'No summary available' }}
            
            ${{ needs.azure-infrastructure-health.outputs.critical_issues && format('**🚨 Critical Issues:** {0}', needs.azure-infrastructure-health.outputs.critical_issues) || '' }}
            
            **Repository:** ${{ github.repository }}
            **Event:** ${{ github.event_name }} ${{ github.event.inputs.check_type && format('({0})', github.event.inputs.check_type) || '' }}
            **Triggered by:** Asif Ali Baig
            
            ${{ github.event.inputs.check_type == 'debug' && needs.azure-infrastructure-health.outputs.debug_info && format('**Debug Info:**\n```\n{0}\n```', needs.azure-infrastructure-health.outputs.debug_info) || '' }}
          color: ${{ needs.azure-infrastructure-health.outputs.status == 'success' && 0x00ff00 || 0xff0000 }}

      - name: Send Kubernetes Infrastructure Notification
        if: needs.kubernetes-health.result != 'skipped'
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ env.DISCORD_WEBHOOK }}
          username: The Sentinel
          status: ${{ needs.kubernetes-health.outputs.status }}
          description: |
            **${{ needs.kubernetes-health.outputs.status == 'success' && '✅ Success' || '❌ Failure' }}: Kubernetes Infrastructure Health Check**
            
            ${{ needs.kubernetes-health.outputs.status == 'failure' && '@everyone **KUBERNETES CLUSTER ISSUE**' || '' }}
            
            **Pod Status**
            ${{ needs.kubernetes-health.outputs.pod_summary || 'No pod information available' }}
            
            **Namespace Details**
            ${{ needs.kubernetes-health.outputs.namespace_details || 'No namespace details available' }}
            
            **Repository:** ${{ github.repository }}
            **Event:** ${{ github.event_name }} ${{ github.event.inputs.check_type && format('({0})', github.event.inputs.check_type) || '' }}
            **Triggered by:** Asif Ali Baig
            
            ${{ github.event.inputs.check_type == 'debug' && needs.kubernetes-health.outputs.debug_info && format('**Debug Info:**\n```\n{0}\n```', needs.kubernetes-health.outputs.debug_info) || '' }}
          color: ${{ needs.kubernetes-health.outputs.status == 'success' && 0x00ff00 || 0xff0000 }}
      - name: Send Monitoring Stack Notification
        if: needs.monitoring-check.result != 'skipped' && needs.monitoring-check.result != ''
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ env.DISCORD_WEBHOOK }}
          username: The Sentinel
          status: ${{ needs.monitoring-check.outputs.status }}
          description: |
            **${{ needs.monitoring-check.outputs.status == 'success' && '✅ Success' || '⚠️ Warning' }}: Monitoring Stack Health Check**
            
            **Monitoring Status**
            ${{ needs.monitoring-check.outputs.monitoring_summary || 'No monitoring information available' }}
            
            **Repository:** ${{ github.repository }}
            **Event:** ${{ github.event_name }} ${{ github.event.inputs.check_type && format('({0})', github.event.inputs.check_type) || '' }}
            **Triggered by:** Asif Ali Baig
          color: ${{ needs.monitoring-check.outputs.status == 'success' && 0x00ff00 || 0xffa500 }}