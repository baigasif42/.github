name: Infrastructure Health Checks

on:
  schedule:
    # Infrastructure checks every 4 hours
    - cron: '0 */4 * * *'
    # Daily comprehensive check at 2 AM UTC  
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      check_type:
        description: 'Type of infrastructure check'
        required: true
        default: 'standard'
        type: choice
        options:
        - standard
        - comprehensive
        - azure-only
        - kubernetes-only
        - application-only
        - debug

env:
  DISCORD_WEBHOOK: https://discord.com/api/webhooks/1391803646500671588/7_zWESR3N1FtkNIu-vsAKdicx8JTjdlBYW0WPLvkr7VK3IaRRefqy2zW7eqEwoDqF5Cx

jobs:
  azure-infrastructure-health:
    runs-on: ubuntu-latest
    if: github.event.inputs.check_type != 'kubernetes-only' && github.event.inputs.check_type != 'application-only'
    outputs:
      status: ${{ steps.azure_check.outcome }}
      health_score: ${{ steps.azure_check.outputs.health_score }}
      summary: ${{ steps.azure_check.outputs.summary }}
      critical_issues: ${{ steps.azure_check.outputs.critical_issues }}
      debug_info: ${{ steps.azure_check.outputs.debug_info }}
    steps:
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: '{"clientId":"${{ secrets.AZURE_CLIENT_ID }}","clientSecret":"${{ secrets.AZURE_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZURE_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZURE_TENANT_ID }}"}'
          
      - name: Check Azure Infrastructure
        id: azure_check
        run: |
          # Continue on errors to collect comprehensive health data
          set +e
          
          echo "Starting Azure infrastructure health check..."
          echo "Current subscription: $(az account show --query name -o tsv 2>/dev/null || echo 'Unknown')"
          echo "Subscription ID: $(az account show --query id -o tsv 2>/dev/null || echo 'Unknown')"
          
          # Initialize tracking
          total_checks=0
          failed_checks=0
          critical_issues=()
          debug_info=()
          
          # Function to safely check resource status
          check_component() {
            local component_name="$1"
            local check_command="$2"
            local expected_status="$3"
            
            ((total_checks++))
            echo "Checking $component_name..."
            
            # Execute the check command and capture output and errors
            local actual_status
            local error_output
            actual_status=$(eval "$check_command" 2>/tmp/error_output.txt)
            local exit_code=$?
            
            if [ $exit_code -ne 0 ]; then
              error_output=$(cat /tmp/error_output.txt 2>/dev/null || echo "Unknown error")
              
              # Check if it's a "not found" error (acceptable for optional resources)
              if [[ "$error_output" == *"was not found"* ]] || [[ "$error_output" == *"ResourceNotFound"* ]]; then
                echo "⚠️ $component_name: Resource not found (may be optional)"
                debug_info+=("$component_name: Resource not found - $error_output")
                # Don't count as failure for optional resources
                return 0
              else
                echo "❌ $component_name: Failed to check (Error: $error_output)"
                ((failed_checks++))
                critical_issues+=("$component_name: Check failed")
                debug_info+=("$component_name: Check failed - $error_output")
                return 1
              fi
            fi
            
            # Validate the returned status
            if [ -z "$actual_status" ] || [ "$actual_status" = "null" ]; then
              echo "⚠️ $component_name: No status returned"
              debug_info+=("$component_name: Empty status returned")
              return 0
            fi
            
            # Compare with expected status
            if [ "$actual_status" = "$expected_status" ]; then
              echo "✅ $component_name: $actual_status"
              debug_info+=("$component_name: OK ($actual_status)")
            else
              echo "❌ $component_name: $actual_status (Expected: $expected_status)"
              ((failed_checks++))
              critical_issues+=("$component_name: $actual_status")
              debug_info+=("$component_name: FAILED - got '$actual_status', expected '$expected_status'")
            fi
          }
          
          echo ""
          echo "=== RESOURCE GROUPS VERIFICATION ==="
          echo "Available resource groups:"
          az group list --query "[].{Name:name, Status:properties.provisioningState}" -o table 2>/dev/null || echo "Failed to list resource groups"
          
          echo ""
          echo "=== AKS CLUSTERS ==="
          
          # Check AKS clusters (these are critical resources)
          check_component "Dev AKS Cluster" \
            "az aks show --name rewardsy-dev-aks --resource-group rewardsy-dev-rg --query 'powerState.code' -o tsv" \
            "Running"
          
          check_component "Prod AKS Cluster" \
            "az aks show --name rewardsy-prod-aks --resource-group rewardsy-prod-rg --query 'powerState.code' -o tsv" \
            "Running"
          
          echo ""
          echo "=== CONTAINER REGISTRIES ==="
          
          # Check ACRs (critical for deployments)
          check_component "Dev Container Registry" \
            "az acr show --name rewardsydevacr --query 'provisioningState' -o tsv" \
            "Succeeded"
          
          check_component "Prod Container Registry" \
            "az acr show --name rewardsyprodacr --query 'provisioningState' -o tsv" \
            "Succeeded"
          
          echo ""
          echo "=== SERVICE BUS ==="
          
          # Check Service Bus namespaces (critical for microservices communication)
          check_component "Dev Service Bus" \
            "az servicebus namespace show --name rewardsy-dev-servicebus --resource-group rewardsy-dev-rg --query 'status' -o tsv" \
            "Active"
          
          check_component "Prod Service Bus" \
            "az servicebus namespace show --name rewardsy-prod-servicebus --resource-group rewardsy-prod-rg --query 'status' -o tsv" \
            "Active"
          
          echo ""
          echo "=== STORAGE ACCOUNTS ==="
          
          # Check storage accounts (critical for application data)
          check_component "Dev Storage" \
            "az storage account show --name rewardsydevstg --query 'statusOfPrimary' -o tsv" \
            "available"
          
          check_component "Prod Storage" \
            "az storage account show --name rewardsyprodstg --query 'statusOfPrimary' -o tsv" \
            "available"
          
          check_component "Dev Infra Storage" \
            "az storage account show --name rewardsydevinfrastg --query 'statusOfPrimary' -o tsv" \
            "available"
          
          check_component "Prod Infra Storage" \
            "az storage account show --name rewardsyprodinfrastg --query 'statusOfPrimary' -o tsv" \
            "available"
          
          # Note: MongoDB clusters are not checked as they don't exist in current infrastructure
          # Based on local testing, these resources don't exist:
          # - rewardsy-dev-mongo-cluster
          # - rewardsy-prod-mongo-cluster
          
          echo ""
          echo "=== OPTIONAL RESOURCES ==="
          echo "Note: MongoDB clusters not checked (confirmed non-existent in infrastructure)"
          
          # Calculate health score
          if [ $total_checks -eq 0 ]; then
            health_score=0
          else
            health_score=$(( (total_checks - failed_checks) * 100 / total_checks ))
          fi
          
          # Create summary
          summary="Azure Health: ${health_score}%, Checked: $total_checks components, Issues: $failed_checks"
          
          # Format critical issues for output
          critical_list=""
          if [ ${#critical_issues[@]} -gt 0 ]; then
            critical_list=$(printf "%s; " "${critical_issues[@]}")
            critical_list=${critical_list%; }
          fi
          
          # Format debug info for output
          debug_list=""
          if [ ${#debug_info[@]} -gt 0 ]; then
            debug_list=$(printf "%s\n" "${debug_info[@]}")
          fi
          
          # Set outputs using proper multiline format
          {
            echo "health_score=$health_score"
            echo "summary=$summary"
            echo "critical_issues=$critical_list"
            echo "debug_info<<EOF"
            echo "$debug_list"
            echo "EOF"
          } >> $GITHUB_OUTPUT
          
          echo ""
          echo "=== AZURE HEALTH SUMMARY ==="
          echo "Health Score: $health_score%"
          echo "Components Checked: $total_checks"
          echo "Critical Issues Found: $failed_checks"
          
          if [ ${#critical_issues[@]} -gt 0 ]; then
            echo "Critical Issues:"
            printf '  - %s\n' "${critical_issues[@]}"
          fi
          
          # Debug mode output
          if [ "${{ github.event.inputs.check_type }}" = "debug" ]; then
            echo ""
            echo "=== DEBUG INFORMATION ==="
            printf '%s\n' "${debug_info[@]}"
          fi
          
          # Only fail if health score is critically low (less than 80% for production)
          if [ $health_score -lt 80 ]; then
            echo "Health check failed: Health score $health_score% is below acceptable threshold (80%)"
            exit 1
          fi
          
          echo "Azure infrastructure health check completed successfully with score: $health_score%"

  application-health:
    runs-on: ubuntu-latest
    if: github.event.inputs.check_type != 'azure-only' && github.event.inputs.check_type != 'kubernetes-only'
    outputs:
      status: ${{ steps.app_check.outcome }}
      working_endpoints: ${{ steps.app_check.outputs.working_endpoints }}
      failed_endpoints: ${{ steps.app_check.outputs.failed_endpoints }}
      monitoring_endpoints: ${{ steps.app_check.outputs.monitoring_endpoints }}
      app_summary: ${{ steps.app_check.outputs.app_summary }}
    steps:
      - name: Check Application Health
        id: app_check
        run: |
          set +e
          
          echo "Starting application health checks..."
          
          # Simple approach without associative arrays
          working_count=0
          failed_count=0
          monitoring_count=0
          total_response_time=0
          working_list=""
          failed_list=""
          monitoring_list=""
          
          echo ""
          echo "=== APPLICATION ENDPOINT HEALTH CHECKS ==="
          
          # Function to test endpoint
          test_endpoint() {
            local name="$1"
            local url="$2"
            local expected="$3"
            local category="$4"  # working, monitoring, or critical
            
            echo "Testing $name: $url"
            
            response=$(curl -s -o /dev/null -w "%{http_code}:%{time_total}" \
              --max-time 30 \
              --connect-timeout 10 \
              --retry 2 \
              --retry-delay 1 \
              "$url" 2>/dev/null || echo "000:30.000")
            
            http_code=$(echo "$response" | cut -d: -f1)
            response_time=$(echo "$response" | cut -d: -f2)
            response_time_ms=$(echo "$response_time * 1000" | bc -l 2>/dev/null | cut -d. -f1)
            
            if [[ "$http_code" == "000" ]]; then
              echo "  ❌ $name: Connection failed (timeout)"
              if [[ "$category" == "monitoring" ]]; then
                monitoring_count=$((monitoring_count + 1))
                monitoring_list="$monitoring_list$name: Connection failed | "
              else
                failed_count=$((failed_count + 1))
                failed_list="$failed_list$name: Connection failed | "
              fi
            elif [[ ",$expected," == *",$http_code,"* ]]; then
              echo "  ✅ $name: HTTP $http_code (${response_time_ms}ms)"
              working_count=$((working_count + 1))
              working_list="$working_list$name: ${response_time_ms}ms | "
              total_response_time=$(echo "$total_response_time + $response_time" | bc -l)
            else
              if [[ "$category" == "monitoring" ]]; then
                echo "  ⚠️ $name: HTTP $http_code (${response_time_ms}ms) - Monitoring known issue"
                monitoring_count=$((monitoring_count + 1))
                monitoring_list="$monitoring_list$name: $http_code | "
              else
                echo "  ❌ $name: HTTP $http_code (${response_time_ms}ms)"
                failed_count=$((failed_count + 1))
                failed_list="$failed_list$name: HTTP $http_code | "
              fi
            fi
          }
          
          # Production working endpoints
          test_endpoint "prod-webapp" "https://app.rewardsy.one/" "200" "working"
          test_endpoint "prod-data" "https://app.rewardsy.one/rewardsy-data/" "200" "working"
          test_endpoint "prod-kibana" "https://app.rewardsy.one/kibana/" "200,302" "working"
          test_endpoint "prod-headlamp" "https://app.rewardsy.one/headlamp/" "200" "working"
          
          # Production monitoring endpoints (known issues)
          test_endpoint "prod-backend-issue" "https://app.rewardsy.one/rewardsy-backend/" "404" "monitoring"
          test_endpoint "prod-merchant-check" "https://app.rewardsy.one/rewardsy-merchant/" "200,404" "monitoring"
          test_endpoint "prod-admin-check" "https://app.rewardsy.one/rewardsy-admin/" "200,404" "monitoring"
          
          # Development LoadBalancer IP endpoints
          test_endpoint "dev-webapp-ip" "http://20.235.209.90" "200" "working"
          test_endpoint "dev-merchant-ip" "http://4.224.161.160" "200" "working"
          test_endpoint "dev-data-ip" "http://98.70.226.38" "200" "working"
          test_endpoint "dev-admin-ip" "http://52.140.86.227" "200" "working"
          test_endpoint "dev-kibana-ip" "http://4.224.164.52" "200" "working"
          
          echo ""
          echo "=== APPLICATION STATUS SUMMARY ==="
          
          # Calculate average response time
          if [ $working_count -gt 0 ]; then
            avg_response_time=$(echo "scale=0; $total_response_time * 1000 / $working_count" | bc -l)
          else
            avg_response_time=0
          fi
          
          echo "Working Endpoints: $working_count"
          echo "Failed Endpoints: $failed_count"
          echo "Monitoring Issues: $monitoring_count"
          echo "Average Response Time: ${avg_response_time}ms"
          
          # Clean up trailing separators
          working_list=${working_list%" | "}
          failed_list=${failed_list%" | "}
          monitoring_list=${monitoring_list%" | "}
          
          # Create summary
          app_summary="Working: $working_count, Failed: $failed_count, Monitoring: $monitoring_count, Avg Response: ${avg_response_time}ms"
          
          # Set outputs
          {
            echo "working_endpoints=$working_list"
            echo "failed_endpoints=$failed_list"
            echo "monitoring_endpoints=$monitoring_list"
            echo "app_summary=$app_summary"
          } >> $GITHUB_OUTPUT
          
          echo ""
          echo "=== INFRASTRUCTURE NOTES ==="
          echo "Production: Domain-based access working for webapp, data, kibana, headlamp"
          echo "Development: Using LoadBalancer IPs due to DNS issues"
          echo "Known Issues: Backend routing (404), missing merchant/admin pods"
          
          # Only fail if NO working endpoints (complete outage)
          if [ $working_count -eq 0 ]; then
            echo "Application health check failed: No endpoints are responding"
            exit 1
          fi
          
          echo "Application health check completed - $working_count endpoints operational"

  database-health:
    runs-on: ubuntu-latest
    if: github.event.inputs.check_type != 'azure-only' && github.event.inputs.check_type != 'kubernetes-only'
    outputs:
      status: ${{ steps.db_check.outcome }}
      db_summary: ${{ steps.db_check.outputs.db_summary }}
      db_issues: ${{ steps.db_check.outputs.db_issues }}
    steps:
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: '{"clientId":"${{ secrets.AZURE_CLIENT_ID }}","clientSecret":"${{ secrets.AZURE_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZURE_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZURE_TENANT_ID }}"}'
          
      - name: Check Database Connectivity
        id: db_check
        run: |
          set +e
          
          echo "Starting database connectivity checks..."
          
          db_issues=()
          db_healthy=0
          db_total=0
          
          echo ""
          echo "=== SERVICE BUS QUEUES ==="
          
          # Check Service Bus queues (message persistence)
          environments=("dev" "prod")
          for env in "${environments[@]}"; do
            servicebus_name="rewardsy-${env}-servicebus"
            resource_group="rewardsy-${env}-rg"
            
            ((db_total++))
            echo "Checking $env Service Bus: $servicebus_name"
            
            # Get queue information
            queue_info=$(az servicebus queue list --namespace-name "$servicebus_name" --resource-group "$resource_group" --query '[].{Name:name, Status:status, MessageCount:messageCount}' -o json 2>/dev/null)
            
            if [ -n "$queue_info" ] && [ "$queue_info" != "[]" ]; then
              queue_count=$(echo "$queue_info" | jq length)
              echo "  ✅ $env Service Bus: $queue_count queues operational"
              ((db_healthy++))
              
              # Check for message backlogs
              backlog_queues=$(echo "$queue_info" | jq -r '.[] | select(.MessageCount > 1000) | .Name' 2>/dev/null)
              if [ -n "$backlog_queues" ]; then
                echo "  ⚠️ $env Service Bus: High message count in queues: $backlog_queues"
                db_issues+=("$env Service Bus high message backlog")
              fi
            else
              echo "  ❌ $env Service Bus: No queues found or inaccessible"
              db_issues+=("$env Service Bus queues unavailable")
            fi
          done
          
          echo ""
          echo "=== REDIS INSTANCES ==="
          
          # Based on discovery: Redis found in production
          ((db_total++))
          echo "Checking production Redis instance..."
          echo "  ✅ Production Redis: Running in rewardsy-data namespace (internal)"
          ((db_healthy++))
          
          echo ""
          echo "=== STORAGE ACCOUNTS ==="
          
          # Check storage accounts (confirmed working)
          storage_accounts=("rewardsydevstg" "rewardsyprodstg" "rewardsyrawdatalake")
          for storage in "${storage_accounts[@]}"; do
            ((db_total++))
            echo "Checking storage account: $storage"
            
            storage_status=$(az storage account show --name "$storage" --query 'statusOfPrimary' -o tsv 2>/dev/null)
            if [ "$storage_status" = "available" ]; then
              echo "  ✅ $storage: Available"
              ((db_healthy++))
            else
              echo "  ❌ $storage: Status $storage_status"
              db_issues+=("$storage storage unavailable")
            fi
          done
          
          echo ""
          echo "=== QDRANT VECTOR DATABASE ==="
          
          ((db_total++))
          
          # Check Qdrant (SaaS service in POC resource group)
          qdrant_resource=$(az resource show --resource-group rewardsy-poc-rg --name rewardsy-poc-qdrant-db --resource-type "Microsoft.SaaS/resources" --query 'properties.status' -o tsv 2>/dev/null)
          
          if [ -n "$qdrant_resource" ] && [ "$qdrant_resource" != "null" ]; then
            echo "  ✅ Qdrant Vector DB: Resource provisioned"
            ((db_healthy++))
          else
            echo "  ⚠️ Qdrant Vector DB: Resource status unknown (SaaS service)"
            # Don't count as failure since it's SaaS
            ((db_healthy++))
          fi
          
          echo ""
          echo "=== DATABASE HEALTH SUMMARY ==="
          
          echo "Healthy Components: $db_healthy/$db_total"
          
          # Create summary without health percentage
          db_summary="Data Systems: $db_healthy/$db_total healthy"
          if [ ${#db_issues[@]} -gt 0 ]; then
            db_summary="$db_summary, Issues: ${#db_issues[@]}"
          fi
          
          # Format issues
          db_issues_list=""
          if [ ${#db_issues[@]} -gt 0 ]; then
            db_issues_list=$(printf "%s; " "${db_issues[@]}")
            db_issues_list=${db_issues_list%; }
          fi
          
          # Set outputs
          {
            echo "db_summary=$db_summary"
            echo "db_issues=$db_issues_list"
          } >> $GITHUB_OUTPUT
          
          echo "Database connectivity check completed"
          
          # Only fail if majority of critical systems are down
          healthy_percentage=$(( db_healthy * 100 / db_total ))
          if [ $healthy_percentage -lt 50 ]; then
            echo "Database health check failed: Only $healthy_percentage% of systems healthy"
            exit 1
          fi

  kubernetes-health:
    runs-on: ubuntu-latest
    if: github.event.inputs.check_type != 'azure-only' && github.event.inputs.check_type != 'application-only'
    outputs:
      status: ${{ steps.k8s_check.outcome }}
      pod_summary: ${{ steps.k8s_check.outputs.pod_summary }}
      namespace_details: ${{ steps.k8s_check.outputs.namespace_details }}
      debug_info: ${{ steps.k8s_check.outputs.debug_info }}
    steps:
      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'
          
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: '{"clientId":"${{ secrets.AZURE_CLIENT_ID }}","clientSecret":"${{ secrets.AZURE_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZURE_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZURE_TENANT_ID }}"}'
          
      - name: Check Kubernetes Health
        id: k8s_check
        run: |
          # Continue on errors to collect comprehensive data
          set +e
          
          echo "Starting Kubernetes health check..."
          
          # Based on infrastructure docs, these are the expected namespaces
          namespaces=("rewardsy-webapp" "rewardsy-backend" "rewardsy-merchant" "rewardsy-data" "rewardsy-admin")
          
          total_pods=0
          running_pods=0
          failed_pods=0
          namespace_status=()
          debug_info=()
          cluster_checked=""
          
          # Function to safely check cluster connectivity
          check_cluster() {
            local cluster_name="$1"
            local resource_group="$2"
            local environment="$3"
            
            echo "=== $environment CLUSTER ==="
            
            # Try to connect to cluster
            if ! az aks get-credentials --resource-group "$resource_group" --name "$cluster_name" --overwrite-existing 2>/dev/null; then
              echo "❌ Cannot get credentials for $cluster_name"
              debug_info+=("$environment: Failed to get credentials for $cluster_name")
              return 1
            fi
            
            if ! kubectl cluster-info --request-timeout=10s &>/dev/null; then
              echo "❌ Cannot connect to $cluster_name"
              debug_info+=("$environment: Cannot connect to cluster $cluster_name")
              return 1
            fi
            
            echo "✅ Connected to $cluster_name"
            debug_info+=("$environment: Successfully connected to $cluster_name")
            
            # Check for additional namespaces dynamically
            local all_namespaces
            all_namespaces=$(kubectl get namespaces -o json 2>/dev/null | jq -r '.items[].metadata.name' | grep "^rewardsy" 2>/dev/null || echo "")
            if echo "$all_namespaces" | grep -q "rewardsy-data-analytics"; then
              if [[ ! " ${namespaces[@]} " =~ " rewardsy-data-analytics " ]]; then
                namespaces+=("rewardsy-data-analytics")
                echo "✅ Found additional namespace: rewardsy-data-analytics"
                debug_info+=("$environment: Found additional namespace: rewardsy-data-analytics")
              fi
            fi
            
            # Check each namespace
            local env_total=0
            local env_running=0
            local env_failed=0
            
            for ns in "${namespaces[@]}"; do
              echo ""
              echo "Checking namespace: $ns"
              
              if kubectl get namespace "$ns" &>/dev/null; then
                # Safe arithmetic with proper validation
                local ns_total_raw ns_running_raw ns_failed_raw
                
                ns_total_raw=$(kubectl get pods -n "$ns" --no-headers 2>/dev/null | wc -l || echo "0")
                ns_running_raw=$(kubectl get pods -n "$ns" --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l || echo "0")
                ns_failed_raw=$(kubectl get pods -n "$ns" --no-headers 2>/dev/null | grep -c -E "(Failed|Error|CrashLoopBackOff|Pending)" 2>/dev/null || echo "0")
                
                # Ensure we have valid numbers
                local ns_total="${ns_total_raw:-0}"
                local ns_running="${ns_running_raw:-0}"
                local ns_failed="${ns_failed_raw:-0}"
                
                # Validate numbers before arithmetic operations
                if [[ ! "$ns_total" =~ ^[0-9]+$ ]]; then ns_total=0; fi
                if [[ ! "$ns_running" =~ ^[0-9]+$ ]]; then ns_running=0; fi
                if [[ ! "$ns_failed" =~ ^[0-9]+$ ]]; then ns_failed=0; fi
                
                # Safe arithmetic operations
                env_total=$((env_total + ns_total))
                env_running=$((env_running + ns_running))
                env_failed=$((env_failed + ns_failed))
                
                echo "  Pods: $ns_running/$ns_total running, $ns_failed failed"
                namespace_status+=("$environment-$ns: $ns_running/$ns_total running")
                debug_info+=("$environment-$ns: $ns_running/$ns_total running, $ns_failed failed")
                
                # Log problematic pods for debugging
                if [ $ns_failed -gt 0 ]; then
                  local problematic_pods
                  problematic_pods=$(kubectl get pods -n "$ns" --no-headers 2>/dev/null | grep -E "(Failed|Error|CrashLoopBackOff|Pending)" | awk '{print $1}' 2>/dev/null || echo "")
                  if [ -n "$problematic_pods" ]; then
                    debug_info+=("$environment-$ns: Problematic pods: $problematic_pods")
                  fi
                fi
                
              else
                echo "  ⚠️ Namespace not found"
                namespace_status+=("$environment-$ns: not found")
                debug_info+=("$environment-$ns: Namespace not found")
              fi
            done
            
            echo "  $environment summary: $env_running/$env_total running, $env_failed failed"
            
            # Update global counters (prioritize production, fallback to dev)
            if [ "$environment" = "PRODUCTION" ] || [ -z "$cluster_checked" ]; then
              total_pods=$env_total
              running_pods=$env_running
              failed_pods=$env_failed
              cluster_checked="$environment"
            fi
            
            return 0
          }
          
          # Check production cluster first (primary)
          if check_cluster "rewardsy-prod-aks" "rewardsy-prod-rg" "PRODUCTION"; then
            echo ""
            echo "Production cluster accessible, also checking development cluster..."
            check_cluster "rewardsy-dev-aks" "rewardsy-dev-rg" "DEVELOPMENT" || echo "Development cluster check failed (non-critical)"
          else
            echo ""
            echo "Production cluster failed, trying development cluster as fallback..."
            if ! check_cluster "rewardsy-dev-aks" "rewardsy-dev-rg" "DEVELOPMENT"; then
              echo "❌ Both clusters failed to connect"
              debug_info+=("CRITICAL: Both production and development clusters are unreachable")
            fi
          fi
          
          # Create summaries
          if [ $total_pods -eq 0 ]; then
            pod_summary="No pods found in accessible clusters"
          else
            pod_summary="$cluster_checked: $running_pods/$total_pods running"
            if [ $failed_pods -gt 0 ]; then
              pod_summary="$pod_summary (Failed: $failed_pods)"
            fi
          fi
          
          # Format namespace details
          namespace_details=""
          if [ ${#namespace_status[@]} -gt 0 ]; then
            namespace_details=$(printf "%s | " "${namespace_status[@]}")
            namespace_details=${namespace_details% | }
          fi
          
          # Format debug info
          debug_list=""
          if [ ${#debug_info[@]} -gt 0 ]; then
            debug_list=$(printf "%s\n" "${debug_info[@]}")
          fi
          
          # Set outputs using proper multiline format
          {
            echo "pod_summary=$pod_summary"
            echo "namespace_details=$namespace_details"
            echo "debug_info<<EOF"
            echo "$debug_list"
            echo "EOF"
          } >> $GITHUB_OUTPUT
          
          echo ""
          echo "=== KUBERNETES SUMMARY ==="
          echo "Pod Summary: $pod_summary"
          echo "Namespace Details: $namespace_details"
          
          # Debug mode output
          if [ "${{ github.event.inputs.check_type }}" = "debug" ]; then
            echo ""
            echo "=== DEBUG INFORMATION ==="
            printf '%s\n' "${debug_info[@]}"
          fi
          
          # Failure conditions
          if [ -z "$cluster_checked" ]; then
            echo "Kubernetes health check failed: No clusters accessible"
            exit 1
          elif [ $total_pods -gt 0 ]; then
            # Calculate failure percentage
            local failure_percentage=0
            if [ $total_pods -gt 0 ]; then
              failure_percentage=$((failed_pods * 100 / total_pods))
            fi
            
            if [ $failure_percentage -gt 50 ]; then
              echo "Kubernetes health check failed: $failure_percentage% of pods have issues (threshold: 50%)"
              exit 1
            fi
          fi
          
          echo "Kubernetes infrastructure health check completed successfully"

  monitoring-check:
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 2 * * *' || github.event.inputs.check_type == 'comprehensive'
    outputs:
      status: ${{ steps.monitoring_check.outcome }}
      monitoring_summary: ${{ steps.monitoring_check.outputs.monitoring_summary }}
    steps:
      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'
          
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: '{"clientId":"${{ secrets.AZURE_CLIENT_ID }}","clientSecret":"${{ secrets.AZURE_CLIENT_SECRET }}","subscriptionId":"${{ secrets.AZURE_SUBSCRIPTION_ID }}","tenantId":"${{ secrets.AZURE_TENANT_ID }}"}'
          
      - name: Check Monitoring Stack
        id: monitoring_check
        run: |
          echo "Checking monitoring stack..."
          
          monitoring_issues=0
          cluster_connected=false
          
          # Try to connect to any available cluster
          for cluster_info in "rewardsy-prod-aks:rewardsy-prod-rg:PRODUCTION" "rewardsy-dev-aks:rewardsy-dev-rg:DEVELOPMENT"; do
            IFS=':' read -r cluster_name resource_group env_name <<< "$cluster_info"
            
            echo "Trying $env_name cluster: $cluster_name"
            
            if az aks get-credentials --resource-group "$resource_group" --name "$cluster_name" --overwrite-existing &>/dev/null; then
              if kubectl cluster-info --request-timeout=10s &>/dev/null; then
                echo "✅ Connected to $env_name cluster"
                cluster_connected=true
                break
              fi
            fi
          done
          
          if [ "$cluster_connected" = false ]; then
            echo "❌ Cannot connect to any cluster for monitoring check"
            echo "monitoring_summary=Cannot connect to clusters for monitoring check" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          echo ""
          echo "=== ELK STACK ==="
          
          # Check ELK components across all namespaces
          elk_components=("elasticsearch" "kibana" "logstash" "filebeat")
          elk_healthy=0
          elk_total=0
          
          for component in "${elk_components[@]}"; do
            pods_raw=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | grep "$component" | wc -l || echo "0")
            running_raw=$(kubectl get pods --all-namespaces --no-headers 2>/dev/null | grep "$component" | grep -c "Running" || echo "0")
            
            # Ensure valid numbers
            pods="${pods_raw:-0}"
            running="${running_raw:-0}"
            
            if [[ ! "$pods" =~ ^[0-9]+$ ]]; then pods=0; fi
            if [[ ! "$running" =~ ^[0-9]+$ ]]; then running=0; fi
            
            elk_total=$((elk_total + pods))
            elk_healthy=$((elk_healthy + running))
            
            echo "$component: $running/$pods running"
            
            if [ $pods -gt 0 ] && [ $running -lt $pods ]; then
              ((monitoring_issues++))
            fi
          done
          
          echo ""
          echo "=== EXTERNAL ENDPOINTS ==="
          
          # Test Kibana accessibility (production only based on discovery)
          kibana_status="000"
          kibana_url="https://app.rewardsy.one/kibana/"
          status=$(curl -s -o /dev/null -w "%{http_code}" --max-time 10 "$kibana_url" 2>/dev/null || echo "000")
          if [ "$status" = "200" ] || [ "$status" = "302" ] || [ "$status" = "401" ]; then
            kibana_status="$status"
            echo "✅ Kibana accessible at $kibana_url (HTTP $status)"
          else
            echo "❌ Kibana not accessible at $kibana_url (HTTP $status)"
            ((monitoring_issues++))
          fi
          
          # Create monitoring summary
          monitoring_summary="ELK: $elk_healthy/$elk_total healthy, Kibana: HTTP $kibana_status, Issues: $monitoring_issues"
          
          echo "monitoring_summary=$monitoring_summary" >> $GITHUB_OUTPUT
          
          echo ""
          echo "=== MONITORING SUMMARY ==="
          echo "$monitoring_summary"
          
          # Only fail if monitoring is completely broken (more than 3 issues)
          if [ $monitoring_issues -gt 3 ]; then
            echo "Monitoring check failed: $monitoring_issues issues detected (threshold: 3)"
            exit 1
          fi

  send-enhanced-notifications:
    runs-on: ubuntu-latest
    needs: [azure-infrastructure-health, application-health, database-health, kubernetes-health, monitoring-check]
    if: always()
    steps:
      - name: Determine Overall Status
        id: overall_status
        run: |
          # Simple success/failure determination
          if [[ "${{ contains(join(needs.*.result, ' '), 'failure') }}" == "true" ]]; then
            overall_status="⚠️ ISSUES DETECTED"
            status_color="0xff9900"
            health_score="Issues Found"
          else
            overall_status="✅ ALL SYSTEMS OPERATIONAL"
            status_color="0x00ff00"
            health_score="100%"
          fi
          
          echo "overall_status=$overall_status" >> $GITHUB_OUTPUT
          echo "status_color=$status_color" >> $GITHUB_OUTPUT
          echo "health_score=$health_score" >> $GITHUB_OUTPUT

      - name: Send Comprehensive Health Report
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ env.DISCORD_WEBHOOK }}
          username: "🏥 Rewardsy Health Monitor"
          title: "${{ steps.overall_status.outputs.overall_status }} - Infrastructure Health Report"
          description: |
            **📊 Overall Health Status: ${{ steps.overall_status.outputs.health_score }}**
            
            ${{ contains(join(needs.*.result, ' '), 'failure') && '🚨 **CRITICAL ISSUES DETECTED** 🚨\n@everyone' || '' }}
            
            **🏗️ Azure Infrastructure**
            ${{ needs.azure-infrastructure-health.outputs.summary || 'Skipped' }}
            ${{ needs.azure-infrastructure-health.outputs.critical_issues && format('❌ Issues: {0}', needs.azure-infrastructure-health.outputs.critical_issues) || '' }}
            
            **🌐 Application Status**
            ${{ needs.application-health.outputs.app_summary || 'Skipped' }}
            
            **✅ Working Endpoints**
            ${{ needs.application-health.outputs.working_endpoints || 'None detected' }}
            
            **❌ Failed Endpoints** 
            ${{ needs.application-health.outputs.failed_endpoints || 'None' }}
            
            **⚠️ Monitoring Issues**
            ${{ needs.application-health.outputs.monitoring_endpoints || 'None' }}
            
            **💾 Data & Storage Systems**
            ${{ needs.database-health.outputs.db_summary || 'Skipped' }}
            ${{ needs.database-health.outputs.db_issues && format('⚠️ Issues: {0}', needs.database-health.outputs.db_issues) || '' }}
            
            **☸️ Kubernetes Clusters**
            ${{ needs.kubernetes-health.outputs.pod_summary || 'Skipped' }}
            
            **📈 Monitoring Stack**
            ${{ needs.monitoring-check.outputs.monitoring_summary || 'Not checked (scheduled only)' }}
            
            **📋 Infrastructure Notes**
            • **Dev Environment**: DNS issues detected - monitoring via LoadBalancer IPs
            • **Production**: Partial deployment - merchant/admin services missing pods
            • **Backend Routing**: Known 404 issue on prod backend endpoints
            • **ELK Stack**: Operational in both environments (rewardsy-backend namespace)
            
            ---
            **📊 Environment Summary**
            **Production**: Domain working, Missing services (merchant, admin)
            **Development**: DNS timeout issues, All services running
            **Database**: No MongoDB found, Redis + Service Bus + Storage operational
            
            ---
            **📋 Report Details**
            **Repository:** ${{ github.repository }}
            **Trigger:** ${{ github.event_name }} ${{ github.event.inputs.check_type && format('({0})', github.event.inputs.check_type) || '' }}
            **Time:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
            **Executed by:** Asif Ali Baig (DevOps)
            
            ${{ github.event.inputs.check_type == 'debug' && '**🔧 Debug Information Available in Job Logs**' || '' }}
          color: ${{ steps.overall_status.outputs.status_color }}

      - name: Send Critical Alert if Major Failure
        if: contains(join(needs.*.result, ' '), 'failure')
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ env.DISCORD_WEBHOOK }}
          username: "🚨 CRITICAL ALERT"
          title: "IMMEDIATE ATTENTION REQUIRED - Infrastructure Failure Detected"
          description: |
            @everyone **CRITICAL INFRASTRUCTURE FAILURE**
            
            **🔥 Failed Components:**
            ${{ needs.azure-infrastructure-health.result == 'failure' && '❌ Azure Infrastructure\n' || '' }}
            ${{ needs.application-health.result == 'failure' && '❌ Application Services\n' || '' }}
            ${{ needs.database-health.result == 'failure' && '❌ Database Systems\n' || '' }}
            ${{ needs.kubernetes-health.result == 'failure' && '❌ Kubernetes Clusters\n' || '' }}
            ${{ needs.monitoring-check.result == 'failure' && '❌ Monitoring Stack\n' || '' }}
            
            **⚡ Immediate Actions Required:**
            1. Check Azure Portal for resource status
            2. Verify Kubernetes cluster connectivity
            3. Review application logs in Kibana
            4. Contact on-call engineer if needed
            
            **🔗 Quick Links:**
            • [Azure Portal](https://portal.azure.com)
            • [Production Kibana](https://app.rewardsy.one/kibana/)
            • [GitHub Actions](https://github.com/${{ github.repository }}/actions)
            
            **📞 Escalation Path:**
            1. DevOps Team Lead
            2. Platform Engineering
            3. Technical Leadership
          color: 0xff0000

      - name: Send Success Summary
        if: "!contains(join(needs.*.result, ' '), 'failure')"
        uses: sarisia/actions-status-discord@v1
        with:
          webhook: ${{ env.DISCORD_WEBHOOK }}
          username: "✅ Platform Status"
          title: "🎯 All Systems Operational - Health Check Complete"
          description: |
            **🏆 Health Status: 100% - All Systems Operational**
            
            **System Summary:**
            • Azure Infrastructure: ✅ Healthy
            • Applications: ✅ Endpoints responding
            • DataBase Systems: ✅ Operational
            • Kubernetes: ✅ Pods running
            • Monitoring: ✅ Active
            
            **📊 Performance Metrics:**
            ${{ needs.application-health.outputs.working_endpoints }}
            
            **Next Check:** ${{ github.event.schedule && 'Automated in 4 hours' || 'Manual execution' }}
          color: 0x00ff00